{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5b07e37",
   "metadata": {},
   "source": [
    "## This Notebook demonstrates how we can consider Latency and Server Availability to optimize the usage of Azure OpenAI\n",
    "\n",
    "### We are considering Azure Monitor Metrics for an Azure service and building up a logic to view the best available region to be used for a given user/subscription. This approach can be further automated and integrated as a Software Repo on GitHub to be used by customers in their applications.\n",
    "\n",
    "This solution addresses the service's latency and availability with the monitoring metrics and then would implement a fallback mechanism to maintain the conversation context and use the most effective region to fetch the responses\n",
    "1. It considers service's availability\n",
    "2. Latency on the service\n",
    "3. Maintains the conversation context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb3236b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import openai\n",
    "import requests, urllib\n",
    "import tiktoken, json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "61f3d3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install opencensus-ext-azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93c3f0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"6dbfc25dd1834836b4ed30b56ea0e933\"\n",
    "\n",
    "openai.api_base = \"https://openai-service4200.openai.azure.com/\"\n",
    "openai.api_key = api_key\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = \"2023-05-15\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d7fa139a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai.Deployment.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b425fee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "response = openai.ChatCompletion.create(\n",
    "  engine=\"mychatgpt\",\n",
    "  messages = [{\"role\":\"system\",\"content\":\"You are an AI assistant that helps people find information.\"},{\"role\":\"user\",\"content\":\"Please help me with top 5 pizza stores in Bengaluru.\"}],\n",
    "  temperature=0.7,\n",
    "  max_tokens=800,\n",
    "  top_p=0.95,\n",
    "  frequency_penalty=0,\n",
    "  presence_penalty=0,\n",
    "  stop=None)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6350e94e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure! Here are the top 5 pizza stores in Bengaluru:\\n \\n 1. California Pizza Kitchen\\n 2. Pizza Hut\\n 3. Domino's Pizza\\n 4. Joey's Pizza\\n 5. Onesta Pizza\\n \\n I hope this helps! Let me know if you need any more assistance.\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# response['choices'][0]['message']['content'].replace('\\n','\\n ').strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527f9083",
   "metadata": {},
   "source": [
    "### Declaring the different regions for OAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da6c9f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining services for multiple regions\n",
    "\n",
    "endpoints = [\"https://openai-service4200.openai.azure.com/\", \"https://openai-scus4200.openai.azure.com/\",\n",
    "             \"https://openai-ncus10.openai.azure.com/\", \"https://oai-frc4200.openai.azure.com/\"]\n",
    "\n",
    "keys = [\"6dbfc25dd1834836b4ed30b56ea0e933\", \"4b706300ad4b479fbe58dd50120ff6be\", \n",
    "        \"612a5d8af3ae471782d4ef7a0d91ef87\", \"437edafaf120410bb0378cab1feb35fe\"]\n",
    "\n",
    "models = [\"mychatgpt\", \"chatgpt\", \"mychatgpt\", \"mychatgpt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ce93017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('https://openai-scus4200.openai.azure.com/',\n",
       " '4b706300ad4b479fbe58dd50120ff6be',\n",
       " 'chatgpt')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoints[1], keys[1], models[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29beee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":  # if there's a name, the role is omitted\n",
    "                num_tokens += -1  # role is always required and always 1 token\n",
    "    num_tokens += 2  # every reply is primed with <im_start>assistant\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab0b71cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = 'Hey there, how can I help you?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "91e56485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_message(messages, model_name, max_response_tokens=500):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=model_name,\n",
    "        messages=messages,\n",
    "        temperature=0.5,\n",
    "        max_tokens=max_response_tokens,\n",
    "        top_p=0.9,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "def print_conversation(messages):\n",
    "    for message in messages:\n",
    "        print(f\"[{message['role'].upper()}]\")\n",
    "        print(message['content'])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73909384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_message = \"I want to write a small blog post about the impact of AI on the future of work.\"\n",
    "user_message = \"I want to buy a tshirt with Avengers printed. Can you suggest some brands?\"\n",
    "# user_message = \"Help me learn about OpenAI. Can you give me a couple of resources for the same?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7467efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"name\":\"manish\", \"content\": user_message}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed4c839a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "token_count = num_tokens_from_messages(messages)\n",
    "print(token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a3869f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 3.562932014465332 seconds ---\n",
      "[{'role': 'system', 'content': 'Hey there, how can I help you?'}, {'role': 'user', 'name': 'manish', 'content': 'I want to buy a tshirt with Avengers printed. Can you suggest some brands?'}, {'role': 'assistant', 'content': 'Sure, there are a lot of brands that sell Avengers printed t-shirts. Here are some popular ones:\\n\\n1. Marvel Official Merchandise\\n2. Adidas\\n3. Under Armour\\n4. H&M\\n5. Zara\\n6. Forever 21\\n7. Hot Topic\\n8. SuperHeroStuff\\n9. BoxLunch\\n10. Target\\n\\nYou can check out their websites or visit their stores to find the perfect Avengers t-shirt for you.'}]\n"
     ]
    }
   ],
   "source": [
    "max_response_tokens = 500\n",
    "\n",
    "# add = \"Can you help me with some store in Gujarat?\"\n",
    "# messages.append({\"role\": \"user\", \"content\": add})\n",
    "model_name = 'mychatgpt'\n",
    "\n",
    "start = time.time()\n",
    "response = send_message(messages, model_name, max_response_tokens)\n",
    "end = time.time()\n",
    "messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (end - start))\n",
    "print(messages)\n",
    "\n",
    "# print(messages)\n",
    "# print_conversation(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad098078",
   "metadata": {},
   "source": [
    "## Maintaining the Context and handling token limits effectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4d431581",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_max_tokens = 4096\n",
    "prompt_max_tokens = overall_max_tokens - max_response_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e67d33b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 385\n",
      "[SYSTEM]\n",
      "Hey there, how can I help you?\n",
      "\n",
      "[USER]\n",
      "I want to write a blog post about the impact of AI on the future of work.\n",
      "\n",
      "[ASSISTANT]\n",
      "Great idea! The impact of AI on the future of work is a topic that has been widely discussed and researched. Here are some key points you could consider including in your blog post:\n",
      "\n",
      "1. Automation: AI is expected to automate many routine and repetitive tasks, which could lead to job displacement for some workers. However, it could also create new job opportunities in fields such as data analysis, programming, and AI development.\n",
      "\n",
      "2. Upskilling: As AI becomes more prevalent in the workplace, it will become increasingly important for workers to develop new skills and adapt to new technologies. This could lead to a greater focus on upskilling and lifelong learning.\n",
      "\n",
      "3. Collaboration: AI is not expected to replace human workers entirely, but rather to work alongside them. This could lead to more collaborative work environments, where humans and machines work together to achieve common goals.\n",
      "\n",
      "4. Ethical considerations: As AI becomes more advanced, it raises important ethical considerations around issues such as privacy, bias, and accountability. It will be important for businesses and governments to address these issues in order to ensure that AI is used in a responsible and ethical manner.\n",
      "\n",
      "5. The future of work: AI is likely to have a significant impact on the way we work in the future, with some experts predicting a shift towards more flexible and remote work arrangements. It will be interesting to see how these changes unfold over time.\n",
      "\n",
      "These are just a few ideas to get you started. Good luck with your blog post!\n",
      "\n",
      "[USER]\n",
      "The target audience for the blog post should be business leaders working in the tech industry.\n",
      "\n",
      "[USER]\n",
      "The target audience for the blog post should be business leaders working in the tech industry.\n",
      "\n",
      "[ASSISTANT]\n",
      "Great, in that case, here are some additional points you could consider including in your blog post:\n",
      "\n",
      "1. Competitive advantage: AI has the potential to give businesses a competitive advantage by enabling them to make more accurate predictions, automate processes, and improve decision-making. Business leaders in the tech industry need to understand how AI can be leveraged to improve their products and services.\n",
      "\n",
      "2. Data-driven decision-making: AI relies on large amounts of data to make accurate predictions and improve performance. Business leaders need to ensure that their organizations have the necessary data infrastructure in place to support AI initiatives.\n",
      "\n",
      "3. Talent acquisition: As AI becomes more prevalent, there will be a growing demand for workers with skills in data analysis, machine learning, and AI development. Business leaders need to think about how they can attract and retain top talent in these fields.\n",
      "\n",
      "4. Investment in AI: Developing and implementing AI initiatives can be expensive, and business leaders need to carefully consider the costs and benefits of investing in AI. They also need to be aware of the risks associated with AI, such as the potential for bias and the impact on jobs.\n",
      "\n",
      "5. Collaboration with academia: AI is a rapidly evolving field, and business leaders in the tech industry need to stay up-to-date with the latest developments. Collaboration with academic institutions can be a valuable way to stay informed and access cutting-edge research.\n",
      "\n",
      "These are just a few additional ideas to consider when writing your blog post for business leaders in the tech industry.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_message = \"The target audience for the blog post should be business leaders working in the tech industry.\"\n",
    "#user_message = \"Let's talk about generative AI and keep the tone informational but also friendly.\"\n",
    "\n",
    "messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "token_count = num_tokens_from_messages(messages)\n",
    "print(f\"Token count: {token_count}\")\n",
    "\n",
    "# remove first message while over the token limit\n",
    "while token_count > prompt_max_tokens:\n",
    "    messages.pop(0)\n",
    "    token_count = num_tokens_from_messages(messages)\n",
    "\n",
    "response = send_message(messages, model_name, max_response_tokens)\n",
    "\n",
    "messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "print_conversation(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843cdbbc",
   "metadata": {},
   "source": [
    "### REST Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d2e7cb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rest_send_message(messages, model_name, max_response_tokens):\n",
    "#     api_url = endpoints[0] + 'openai/deployments/'+model_name+'/chat/completions?api-version=2023-05-15'\n",
    "    \n",
    "#     headers =  {\"Content-Type\":\"application/json\", \"api-key\": keys[0]}\n",
    "#     messages = json.dumps(messages)\n",
    "    \n",
    "#     response = requests.post(api_url, data=json.dumps(messages), headers=headers)\n",
    "    \n",
    "#     return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "4ee5aa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_message = \"Can you help me fetch a pizza place in India?\"\n",
    "# # user_message = \"The target audience for the blog post should be business leaders working in the tech industry.\"\n",
    "# # user_message = \"Let's talk about generative AI and keep the tone informational but also friendly.\"\n",
    "# messages=[\n",
    "#     {\"role\": \"system\", \"content\": system_message},\n",
    "#     {\"role\": \"user\", \"name\":\"manish\", \"content\": user_message}\n",
    "# ]\n",
    "\n",
    "# # messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "# token_count = num_tokens_from_messages(messages)\n",
    "# print(f\"Token count: {token_count}\")\n",
    "\n",
    "# # remove first message while over the token limit\n",
    "# while token_count > prompt_max_tokens:\n",
    "#     messages.pop(0)\n",
    "#     token_count = num_tokens_from_messages(messages)\n",
    "\n",
    "# response = rest_send_message(messages, model_name, max_response_tokens)\n",
    "# # print(response)\n",
    "# messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "# print_conversation(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe77dd6",
   "metadata": {},
   "source": [
    "## Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ee1fba7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For openai-service4200 latency is 3.18 sec\n",
      "For openai-scus4200 latency is 3.58 sec\n",
      "For openai-ncus10 latency is 4.23 sec\n",
      "For oai-frc4200 latency is 8.6 sec\n"
     ]
    }
   ],
   "source": [
    "latency_values = []\n",
    "start = time.time()\n",
    "for endpoint in endpoints:\n",
    "    service_name = endpoint.split('//')[1].split('.')[0]\n",
    "#     print(service_name)\n",
    "    url = \"https://management.azure.com/subscriptions/ef245f14-9dca-41c6-ab0f-12e5afe4692b/resourceGroups/openai-rg/providers/Microsoft.CognitiveServices/accounts/\"+service_name+\"/providers/microsoft.insights/metrics?metricnames=Latency&api-version=2019-07-01&timespan=2023-09-17/2023-09-19&top=3&aggregation=Average&interval=PT1M\"\n",
    "\n",
    "    headers =  {\"Content-Type\":\"application/json\", \"Authorization\": \"Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsIng1dCI6Ii1LSTNROW5OUjdiUm9meG1lWm9YcWJIWkdldyIsImtpZCI6Ii1LSTNROW5OUjdiUm9meG1lWm9YcWJIWkdldyJ9.eyJhdWQiOiJodHRwczovL21hbmFnZW1lbnQuYXp1cmUuY29tIiwiaXNzIjoiaHR0cHM6Ly9zdHMud2luZG93cy5uZXQvNzJmOTg4YmYtODZmMS00MWFmLTkxYWItMmQ3Y2QwMTFkYjQ3LyIsImlhdCI6MTY5NTAxODEwNywibmJmIjoxNjk1MDE4MTA3LCJleHAiOjE2OTUxMDQ4MDcsImFpbyI6IkUyRmdZTkJ1Q05FdldiWlc2bDdYcmg4cjR6NCtBUUE9IiwiYXBwaWQiOiI2ZmQxNzllOS01Nzg5LTRiNDAtYmVkMC0xZThkYWYzYTU0ZTAiLCJhcHBpZGFjciI6IjEiLCJpZHAiOiJodHRwczovL3N0cy53aW5kb3dzLm5ldC83MmY5ODhiZi04NmYxLTQxYWYtOTFhYi0yZDdjZDAxMWRiNDcvIiwiaWR0eXAiOiJhcHAiLCJvaWQiOiJiMjU2NjFjZS1kODljLTQ3ZWMtOTJiYi02ZjQ2MmRkNTJkZjkiLCJyaCI6IjAuQVFFQXY0ajVjdkdHcjBHUnF5MTgwQkhiUjBaSWYza0F1dGRQdWtQYXdmajJNQk1hQUFBLiIsInN1YiI6ImIyNTY2MWNlLWQ4OWMtNDdlYy05MmJiLTZmNDYyZGQ1MmRmOSIsInRpZCI6IjcyZjk4OGJmLTg2ZjEtNDFhZi05MWFiLTJkN2NkMDExZGI0NyIsInV0aSI6ImJoZnN5VlJjUEUtVFgyVmJ3ZDBMQUEiLCJ2ZXIiOiIxLjAiLCJ4bXNfdGNkdCI6MTI4OTI0MTU0N30.inp_wQQgsx4ixyc3U6I7yfJb67jFJrDghtHs_ol_7sq0W5TiT582qnYJpJ847qPER-MsXoH8j6FIUzn6l-0j7JG6v3oNBY49vQ0TPLhq5C1ogXuSAu_oG-K2jqAvrQ_-DeTfezkSbzxctg_TAP_67gt4iH4sfi1mkvUCQMJ9xIN-iGEjtM_hsj_ykK87aNGlChoWTPYx5I8cFI0dS5YB2y3F058VbIt0GtXa5ufLG_B2eYKCQFE1Td_BUybQulw2geROSByirTNAOrtZOzZwmhuYH23imFpYHAvhZM5RYxFcr1KiB8Ph2AOmVmRQmx_RfWfdKcp4YlFWGxpixDB9BA\"}\n",
    "    # messages = json.dumps(messages)\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # print(response.json()['value'][0]['timeseries'][0]['data'])\n",
    "    latencies=[]\n",
    "    for items in response.json()['value'][0]['timeseries'][0]['data'][:]:\n",
    "        if len(items)>1:\n",
    "    #         print(items)\n",
    "            latencies.append(items['average'])\n",
    "#         np.round(np.average(latencies),2)\n",
    "    print(\"For\",service_name, \"latency is\", np.round(np.average(latencies)/1000,2),\"sec\")\n",
    "    latency_values.append(str(np.round(np.average(latencies)/1000,2))+\" seconds\")\n",
    "end = time.time()\n",
    "\n",
    "# print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e14855",
   "metadata": {},
   "source": [
    "## Server Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "31bec9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan Server Errors\n",
      "nan Server Errors\n",
      "nan Server Errors\n",
      "nan Server Errors\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for endpoint in endpoints:\n",
    "    service_name = endpoint.split('//')[1].split('.')[0]\n",
    "#     print(service_name)\n",
    "    url = \"https://management.azure.com/subscriptions/ef245f14-9dca-41c6-ab0f-12e5afe4692b/resourceGroups/openai-rg/providers/Microsoft.CognitiveServices/accounts/\"+service_name+\"/providers/microsoft.insights/metrics?metricnames=ServerErrors&api-version=2019-07-01&timespan=2023-09-17/2023-09-19&top=3&aggregation=Average&interval=PT1H\"\n",
    "\n",
    "    headers =  {\"Content-Type\":\"application/json\", \"Authorization\": \"Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsIng1dCI6Ii1LSTNROW5OUjdiUm9meG1lWm9YcWJIWkdldyIsImtpZCI6Ii1LSTNROW5OUjdiUm9meG1lWm9YcWJIWkdldyJ9.eyJhdWQiOiJodHRwczovL21hbmFnZW1lbnQuYXp1cmUuY29tIiwiaXNzIjoiaHR0cHM6Ly9zdHMud2luZG93cy5uZXQvNzJmOTg4YmYtODZmMS00MWFmLTkxYWItMmQ3Y2QwMTFkYjQ3LyIsImlhdCI6MTY5NTAxODEwNywibmJmIjoxNjk1MDE4MTA3LCJleHAiOjE2OTUxMDQ4MDcsImFpbyI6IkUyRmdZTkJ1Q05FdldiWlc2bDdYcmg4cjR6NCtBUUE9IiwiYXBwaWQiOiI2ZmQxNzllOS01Nzg5LTRiNDAtYmVkMC0xZThkYWYzYTU0ZTAiLCJhcHBpZGFjciI6IjEiLCJpZHAiOiJodHRwczovL3N0cy53aW5kb3dzLm5ldC83MmY5ODhiZi04NmYxLTQxYWYtOTFhYi0yZDdjZDAxMWRiNDcvIiwiaWR0eXAiOiJhcHAiLCJvaWQiOiJiMjU2NjFjZS1kODljLTQ3ZWMtOTJiYi02ZjQ2MmRkNTJkZjkiLCJyaCI6IjAuQVFFQXY0ajVjdkdHcjBHUnF5MTgwQkhiUjBaSWYza0F1dGRQdWtQYXdmajJNQk1hQUFBLiIsInN1YiI6ImIyNTY2MWNlLWQ4OWMtNDdlYy05MmJiLTZmNDYyZGQ1MmRmOSIsInRpZCI6IjcyZjk4OGJmLTg2ZjEtNDFhZi05MWFiLTJkN2NkMDExZGI0NyIsInV0aSI6ImJoZnN5VlJjUEUtVFgyVmJ3ZDBMQUEiLCJ2ZXIiOiIxLjAiLCJ4bXNfdGNkdCI6MTI4OTI0MTU0N30.inp_wQQgsx4ixyc3U6I7yfJb67jFJrDghtHs_ol_7sq0W5TiT582qnYJpJ847qPER-MsXoH8j6FIUzn6l-0j7JG6v3oNBY49vQ0TPLhq5C1ogXuSAu_oG-K2jqAvrQ_-DeTfezkSbzxctg_TAP_67gt4iH4sfi1mkvUCQMJ9xIN-iGEjtM_hsj_ykK87aNGlChoWTPYx5I8cFI0dS5YB2y3F058VbIt0GtXa5ufLG_B2eYKCQFE1Td_BUybQulw2geROSByirTNAOrtZOzZwmhuYH23imFpYHAvhZM5RYxFcr1KiB8Ph2AOmVmRQmx_RfWfdKcp4YlFWGxpixDB9BA\"}\n",
    "    # messages = json.dumps(messages)\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # print(response.json()['value'][0]['timeseries'][0]['data'])\n",
    "    latencies=[]\n",
    "    for items in response.json()['value'][0]['timeseries'][0]['data'][:]:\n",
    "        if items['average']>0:\n",
    "    #         print(items)\n",
    "            latencies.append(items['average'])\n",
    "    print(np.average(latencies), \"Server Errors\")\n",
    "end = time.time()\n",
    "\n",
    "# print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1d4f3f",
   "metadata": {},
   "source": [
    "## Verifying the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c2c02dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 5.634791135787964 seconds --- for openai-service4200\n",
      "--- 5.188538312911987 seconds --- for openai-scus4200\n",
      "--- 4.028377056121826 seconds --- for openai-ncus10\n",
      "--- 11.607852458953857 seconds --- for oai-frc4200\n"
     ]
    }
   ],
   "source": [
    "output_time = []\n",
    "for i in range(len(endpoints)):\n",
    "    \n",
    "    user_message = \"Please give me a short recipe for making a Veggie Pizza.\"\n",
    "\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"name\":\"manish\", \"content\": user_message}\n",
    "    ]\n",
    "\n",
    "    service_name = endpoints[i].split('//')[1].split('.')[0]\n",
    "    openai.api_base = endpoints[i]\n",
    "    openai.api_key = keys[i]\n",
    "    model_name = models[i]\n",
    "    \n",
    "    max_response_tokens = 500\n",
    "\n",
    "    start = time.time()\n",
    "    response = send_message(messages, model_name, max_response_tokens)\n",
    "    end = time.time()\n",
    "    messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "\n",
    "    print(\"--- %s seconds ---\" % (end - start), \"for\", service_name)\n",
    "    val = np.round(end - start,2)\n",
    "    output_time.append(str(val) + ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a1981705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['3.18 seconds', '3.58 seconds', '4.23 seconds', '8.6 seconds'],\n",
       " ['5.63 seconds', '5.19 seconds', '4.03 seconds', '11.61 seconds'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latency_values, output_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "37c52184",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = [\"East US\", \"South Central US\", \"North Central US\", \"France Central\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "594cd362",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = []\n",
    "\n",
    "for j in range(len(latency_values)):\n",
    "    compare.append([endpoints[j].split('//')[1].split('.')[0], latency_values[j], output_time[j], regions[j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "af09dac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------------------------+------------------+\n",
      "| Service Name       | Regional Latency   | Response Time for a prompt   | Region           |\n",
      "+====================+====================+==============================+==================+\n",
      "| openai-service4200 | 3.18 seconds       | 5.63 seconds                 | East US          |\n",
      "+--------------------+--------------------+------------------------------+------------------+\n",
      "| openai-scus4200    | 3.58 seconds       | 5.19 seconds                 | South Central US |\n",
      "+--------------------+--------------------+------------------------------+------------------+\n",
      "| openai-ncus10      | 4.23 seconds       | 4.03 seconds                 | North Central US |\n",
      "+--------------------+--------------------+------------------------------+------------------+\n",
      "| oai-frc4200        | 8.6 seconds        | 11.61 seconds                | France Central   |\n",
      "+--------------------+--------------------+------------------------------+------------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    " \n",
    "# display table\n",
    "head = ['Service Name', 'Regional Latency', 'Response Time for a prompt', 'Region']\n",
    "print(tabulate(compare, headers=head, tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db32000",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
